{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.13", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 0, 
  "cells": [
    {
      "source": [
        "# More Classification"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "hide": true
      }
    }, 
    {
      "source": [
        "$$\n", 
        "\\renewcommand{\\like}{{\\cal L}}\n", 
        "\\renewcommand{\\loglike}{{\\ell}}\n", 
        "\\renewcommand{\\err}{{\\cal E}}\n", 
        "\\renewcommand{\\dat}{{\\cal D}}\n", 
        "\\renewcommand{\\hyp}{{\\cal H}}\n", 
        "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n", 
        "\\renewcommand{\\x}{{\\mathbf x}}\n", 
        "\\renewcommand{\\v}[1]{{\\mathbf #1}}\n", 
        "$$"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "variables": {
          "\\cal D": {}, 
          "\\cal E": {}, 
          "\\mathbf #1": {}, 
          "\\mathbf x": {}, 
          "\\cal L": {}, 
          "\\cal H": {}, 
          "\\ell": {}
        }, 
        "hide": true
      }
    }, 
    {
      "execution_count": 2, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "pd.set_option('display.width', 500)\n", 
        "pd.set_option('display.max_columns', 100)\n", 
        "pd.set_option('display.notebook_repr_html', True)\n", 
        "import seaborn as sns\n", 
        "sns.set_style(\"whitegrid\")\n", 
        "sns.set_context(\"poster\")\n", 
        "from PIL import Image"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "execution_count": 3, 
      "cell_type": "code", 
      "source": [
        "c0=sns.color_palette()[0]\n", 
        "c1=sns.color_palette()[1]\n", 
        "c2=sns.color_palette()[2]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "from matplotlib.colors import ListedColormap\n", 
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n", 
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n", 
        "cm = plt.cm.RdBu\n", 
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n", 
        "\n", 
        "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.1, psize=10, zfunc=False, predicted=False):\n", 
        "    h = .02\n", 
        "    X=np.concatenate((Xtr, Xte))\n", 
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n", 
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n", 
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n", 
        "                         np.linspace(y_min, y_max, 100))\n", 
        "\n", 
        "    #plt.figure(figsize=(10,6))\n", 
        "    if zfunc:\n", 
        "        p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n", 
        "        p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", 
        "        Z=zfunc(p0, p1)\n", 
        "    else:\n", 
        "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n", 
        "    ZZ = Z.reshape(xx.shape)\n", 
        "    if mesh:\n", 
        "        plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light, alpha=alpha, axes=ax)\n", 
        "    if predicted:\n", 
        "        showtr = clf.predict(Xtr)\n", 
        "        showte = clf.predict(Xte)\n", 
        "    else:\n", 
        "        showtr = ytr\n", 
        "        showte = yte\n", 
        "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n", 
        "    # and testing points\n", 
        "    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n", 
        "    ax.set_xlim(xx.min(), xx.max())\n", 
        "    ax.set_ylim(yy.min(), yy.max())\n", 
        "    return ax,xx,yy"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true, 
        "hide": true
      }
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1):\n", 
        "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, colorscale=colorscale, cdiscrete=cdiscrete, psize=psize, alpha=alpha, predicted=True) \n", 
        "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", 
        "    Z = Z.reshape(xx.shape)\n", 
        "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n", 
        "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n", 
        "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14, axes=ax)\n", 
        "    return ax "
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true, 
        "hide": true
      }
    }, 
    {
      "source": [
        "## The ATM Camera example"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Imagine that you are tasked whith making a smart ATM camera which can distinguish between dollar notes and checks. You want to make sure that dollars are not classified as checks, and that checks are not classified as dollars.\n", 
        "\n", 
        "You are given a set of 87 images of checks and dollars, each of which have been scaled to 322 X 137 pixels, and where each pixel has 3 color channels.\n", 
        "\n", 
        "`imag.pix.npy` is 80M. Download it from https://dl.dropboxusercontent.com/u/75194/imag.pix.npy and copy it to the data subfolder."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "data=np.load(\"data/imag.pix.npy\")\n", 
        "y=np.load(\"data/imag.lbl.npy\")\n", 
        "STANDARD_SIZE = (322, 137)#standardized pixels in image.\n", 
        "data.shape, y.shape"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 9, 
      "cell_type": "code", 
      "source": [
        "def get_image(mat):\n", 
        "    size = STANDARD_SIZE[0]*STANDARD_SIZE[1]*3\n", 
        "    r,g,b = mat[0:size:3], mat[1:size:3],mat[2:size:3]\n", 
        "    rgbArray = np.zeros((STANDARD_SIZE[1],STANDARD_SIZE[0], 3), 'uint8')#3 channels\n", 
        "    rgbArray[..., 0] = r.reshape((STANDARD_SIZE[1], STANDARD_SIZE[0]))\n", 
        "    rgbArray[..., 1] = b.reshape((STANDARD_SIZE[1], STANDARD_SIZE[0]))\n", 
        "    rgbArray[..., 2] = g.reshape((STANDARD_SIZE[1], STANDARD_SIZE[0]))\n", 
        "    return rgbArray\n", 
        "\n", 
        "def display_image(mat):\n", 
        "    with sns.axes_style(\"white\"):\n", 
        "        plt.imshow(get_image(mat))\n", 
        "        plt.xticks([])\n", 
        "        plt.yticks([])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We display some of the images that we have:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 10, 
      "cell_type": "code", 
      "source": [
        "display_image(data[5])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "execution_count": 11, 
      "cell_type": "code", 
      "source": [
        "display_image(data[50])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "What do you think are some of the aspects of these images that will help us distinguish checks from dollars?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The curse of dimensionality: Feature engineering"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The first thing that you notice is that you have many many features: to be precise, $322 x 137 x 3 = 136452$ of them. This is a lot of features! Having too many features can lead to overfitting.\n", 
        "\n", 
        "You have seem this before! Remember when we did the polynomial regression? When we tried to find fits in $\\hyp_1$, there were two features, the constant, and $x$. In $\\hyp_2$, there are 3: the constant, $x$, and $x^2$. When we get to $\\hyp_{20}$, there are 21: the constant, and 20 powers of x. And then we saw how regularization tried to eliminate some of these powers by sending their co-efficients in the polynomial very close to 0, thus reducing the number of features we had.\n", 
        "\n", 
        "Another way to look at this problem is the following: we have 85 data points, but 136452 features; that is, way more features than data points. Thus there is a high chance that a few attributes will correlate with $y$ purely coincidentally!\n", 
        "[^Having lots of images, or \"big-data\" helps in combatting overfitting!]\n", 
        "\n", 
        "We need to do something similar to what happened in the regularized regression here! We will engage in some *a-priori* feature selection that will reduce the dimensionality of the problem. The idea we'll use here is something called **Principal Components Analysis**, or PCA.\n", 
        "\n", 
        "PCA is an unsupervized learning technique. The basic idea behind PCA is to rotate the co-ordinate axes of the feature space. We first find the direction in which the data varies the most. We set up one co-ordinate axes along this direction, which is called the first principal component. We then look for a perpendicular direction in which the data varies the second most. This is the second principal component. The diagram illustrates this process. There are as many principal components as the feature dimension: all we have done is a rotation.\n", 
        "\n", 
        "![pcanim](images/pcanim.gif)\n", 
        "\n", 
        "(diagram taken from http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues which also has nice discussions)\n", 
        "\n", 
        "How does this then achieve feature selection? We decide on a threshold of variation; once the variation in a particular direction falls below a certain number, we get rid of all the co-ordinate axes after that principal component. For example, if the variation falls below 10% after the third axes, and we decide that 10% is an acceptable cutoff, we remove all domensions from the fourth dimension onwards. In other words, we took our higher dimensional problem and projected it onto a 3 dimensional **subspace**.\n", 
        "\n", 
        "We do not have to do this dimensionality reduction unsupervized. Indeed, you will see some supervized dimensionality reduction in the homework.\n", 
        "\n", 
        "These two ideas illustrate one of the most important reasons that learning is even feasible: we believe that **most datasets, in either their unsupervized form $\\{\\v{x\\}}$, or their supervized form $\\{y, \\v{x}\\}$, live on a lower dimensional subspace**. If we can find this subspace, we can then hope to find a methodd which rerpectively separates or fits the data.\n", 
        "\n", 
        "Here we'll continue to focus on PCA. We'll reduce our dimensionality from 136452 to 60. We choose 60 as a large apriori number: we dont know if the variation in the data will have gone below a reasonable threshold by then. Notice that we use `fit_transform` in the `sklearn` API which takes the original 87 rows x 136452 columns dimensional data `data` and transforms it to a 87 x 90 data matrix `X`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 12, 
      "cell_type": "code", 
      "source": [
        "from sklearn.decomposition import PCA\n", 
        "pca = PCA(n_components=60)\n", 
        "X = pca.fit_transform(data)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 13, 
      "cell_type": "code", 
      "source": [
        "print(pca.explained_variance_ratio_.sum())"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The explained variance ratio `pca.explained_variance_ratio_` tells us how much of the variation in the features is explained by these 60 features. When we sum it up over the features, we see that 94% is explained: good enough to go down to a 60 dimensional space from a 136452 dimensional one!\n", 
        "\n", 
        "We can see the individual variances as we increase the dimensionality:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 14, 
      "cell_type": "code", 
      "source": [
        "pca.explained_variance_ratio_*100"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The first dimension accounts for 35% of the variation, the second 6%, and it goes steadily down from there.\n", 
        "\n", 
        "Let us create a dataframe with these 60 features labelled pc1,pc2...,pc60 and the labels of the sample:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 15, 
      "cell_type": "code", 
      "source": [
        "df = pd.DataFrame({\"y\":y, \"label\":np.where(y==1, \"check\", \"dollar\")})\n", 
        "for i in range(pca.explained_variance_ratio_.shape[0]):\n", 
        "    df[\"pc%i\" % (i+1)] = X[:,i]\n", 
        "df.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets see what these principal components look like:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 16, 
      "cell_type": "code", 
      "source": [
        "def normit(a):\n", 
        "    a=(a - a.min())/(a.max() -a.min())\n", 
        "    a=a*256\n", 
        "    return np.round(a)\n", 
        "def getNC(pc, j):\n", 
        "    size=322*137*3\n", 
        "    r=pc.components_[j][0:size:3]\n", 
        "    g=pc.components_[j][1:size:3]\n", 
        "    b=pc.components_[j][2:size:3]\n", 
        "    r=normit(r)\n", 
        "    g=normit(g)\n", 
        "    b=normit(b)\n", 
        "    return r,g,b\n", 
        "def display_component(pc, j):\n", 
        "    r,g,b = getNC(pc,j)\n", 
        "    rgbArray = np.zeros((137,322,3), 'uint8')\n", 
        "    rgbArray[..., 0] = r.reshape(137,322)\n", 
        "    rgbArray[..., 1] = g.reshape(137,322)\n", 
        "    rgbArray[..., 2] = b.reshape(137,322)\n", 
        "    plt.imshow(rgbArray)\n", 
        "    plt.xticks([])\n", 
        "    plt.yticks([])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 17, 
      "cell_type": "code", 
      "source": [
        "display_component(pca,0)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "execution_count": 18, 
      "cell_type": "code", 
      "source": [
        "display_component(pca,1)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We take the first two principal components and immediately notice in the diagram below that they are enough to separate out the checks and the dollars. Indeed the first component itself seems to be mostly enough. We can look at the image of the first component and speculate that the medallion in the middle of the dollars probably contributes to this."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 19, 
      "cell_type": "code", 
      "source": [
        "colors = [c0, c2]\n", 
        "for label, color in zip(df['label'].unique(), colors):\n", 
        "    mask = df['label']==label\n", 
        "    plt.scatter(df[mask]['pc1'], df[mask]['pc2'], c=color, label=label)\n", 
        "plt.legend()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "You might be a bit confused: we needed to use 60 components to explain 94% of the variation in the features, but only 1 or 2 components to separate checks from dollars? This is because PCA is unsupervised: the only variation we are explaining is the variation in the 136452 dimensional feature space. We are not explaining the variation in the $y$ or the label, and it might turn out, as it does in this case, that with the additional information in $y$, the dimensionality needed for classification is much lower.\n", 
        "\n", 
        "We could thus choose just the first few principal components to make our classifier. For the purposes of this lab, since two components can be easily visualized (even though adding some fore features may leads to better separability), we'll go with learning a 2-dimensional classifier in the `pc1` and `pc2` dimensions! "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Setting up some code"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 20, 
      "cell_type": "code", 
      "source": [
        "from sklearn.grid_search import GridSearchCV\n", 
        "def cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=5):\n", 
        "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n", 
        "    gs.fit(Xtrain, ytrain)\n", 
        "    print(\"BEST PARAMS\", gs.best_params_)\n", 
        "    best = gs.best_estimator_\n", 
        "    return best"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 21, 
      "cell_type": "code", 
      "source": [
        "from sklearn.cross_validation import train_test_split\n", 
        "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, standardize=False, train_size=0.8, sets=None):\n", 
        "    if sets:\n", 
        "        Xtrain, Xtest, ytrain, ytest = sets['Xtrain'], sets['Xtest'], sets['ytrain'], sets['ytest']\n", 
        "    else:       \n", 
        "        subdf=indf[featurenames]\n", 
        "        #X=subdf.values\n", 
        "        y=(indf[targetname].values==target1val)*1\n", 
        "        #Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size)\n", 
        "        itrain, itest = train_test_split(range(subdf.shape[0]), train_size=train_size)\n", 
        "        if standardize:\n", 
        "            dftrain=(subdf.iloc[itrain] - subdf.iloc[itrain].mean())/subdf.iloc[itrain].std()\n", 
        "            dftest=(subdf.iloc[itest] - subdf.iloc[itest].mean())/subdf.iloc[itest].std()\n", 
        "        else:\n", 
        "            dftrain=subdf.iloc[itrain]\n", 
        "            dftest=subdf.iloc[itest]\n", 
        "        Xtrain, Xtest, ytrain, ytest = dftrain.values, dftest.values, y[itrain], y[itest]\n", 
        "    clf = cv_optimize(clf, parameters, Xtrain, ytrain)\n", 
        "    clf=clf.fit(Xtrain, ytrain)\n", 
        "    training_accuracy = clf.score(Xtrain, ytrain)\n", 
        "    test_accuracy = clf.score(Xtest, ytest)\n", 
        "    print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n", 
        "    print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n", 
        "    return clf, Xtrain, ytrain, Xtest, ytest"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true, 
        "hide": true
      }
    }, 
    {
      "source": [
        "### Cross-validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "> YOUR TURN NOW\n", 
        "Lets carry out a cross-validation.  We plot the results in the diagram below. The results are fairly stable and correspond to our intuition that the first principal component basically separates the data."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 24, 
      "cell_type": "code", 
      "source": [
        "#bestcv, Xtrain, ytrain, Xtest, ytest = ...\n", 
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 25, 
      "cell_type": "code", 
      "source": [
        "plt.figure()\n", 
        "ax=plt.gca()\n", 
        "points_plot(ax, Xtrain, Xtest, ytrain, ytest, bestcv, alpha=0.5, psize=20);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "We can plot the probability contours as well: the probability is easily obtained by just counting the fraction of neighbors that are blue or red."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 26, 
      "cell_type": "code", 
      "source": [
        "plt.figure()\n", 
        "ax=plt.gca()\n", 
        "points_plot_prob(ax, Xtrain, Xtest, ytrain, ytest, bestcv, alpha=0.5, psize=20);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "#### Evaluation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 27, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n", 
        "confusion_matrix(ytest, bestcv.predict(Xtest), )"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## The confusion matrix: comparing classifiers"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We have written two classifiers. A classifier will get some samples right, and some wrong. Generally we see which ones it gets right and which ones it gets wrong on the test set. There,\n", 
        "\n", 
        "- the samples that are +ive and the classifier predicts as +ive are called True Positives (TP)\n", 
        "- the samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP)\n", 
        "- the samples that are -ive and the classifier predicts as -ive are called True Negatives (TN)\n", 
        "- the samples that are +ive and the classifier predicts as -ive are called False Negatives (FN)\n", 
        "\n", 
        "A classifier produces a confusion matrix from these which lookslike this:\n", 
        "\n", 
        "![hwimages](./images/confusionmatrix.png)\n", 
        "\n", 
        "\n", 
        "IMPORTANT NOTE: In sklearn, to obtain the confusion matrix in the form above, always have the observed `y` first, i.e.: use as `confusion_matrix(y_true, y_pred)`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Given these definitions, we typically calculate a few metrics for our classifier. First, the **True Positive Rate**:\n", 
        "\n", 
        "$$TPR = Recall = \\frac{TP}{OP} = \\frac{TP}{TP+FN},$$\n", 
        "\n", 
        "also called the Hit Rate: the fraction of observed positives (1s) the classifier gets right, or how many true positives were recalled. Maximizing the recall towards 1 means keeping down the false negative rate. In a classifier try to find cancer patients, this is the number we want to maximize.\n", 
        "\n", 
        "The **False Positive Rate** is defined as\n", 
        "\n", 
        "$$FPR = \\frac{FP}{ON} = \\frac{FP}{FP+TN},$$\n", 
        "\n", 
        "also called the False Alarm Rate, the fraction of observed negatives (0s) the classifier gets wrong. In general, you want this number to be low. Instead, you might want to maximize the\n", 
        "**Precision**,which tells you how many of the predicted positive(1) hits were truly positive\n", 
        "\n", 
        "$$Precision = \\frac{TP}{PP} = \\frac{TP}{TP+FP}.$$\n", 
        "\n", 
        "Finally the **F1** score gives us the Harmonic Score of Precision and Recall. Many analysts will try and find a classifier that maximizes this score, since it tries to minimize both false positives and false negatives simultaneously, and is thus a bit more precise in what it is trying to do than the accuracy.\n", 
        "\n", 
        "$$F1 =  \\frac{2*Recall*Precision}{Recall + Precision}$$\n", 
        "\n", 
        "However, in a case like that of a cancer classifier, we will wish to minimize false nagatives at the expense of false positives: it is ok to send perfectly healthy patients for cancer folloup if that is the price we must pay for not missing any sick ones.\n", 
        "\n", 
        "`scikit-learn` helpfully gives us a classification report with all these numbers"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### The cancer doctor\n", 
        "\n", 
        "Do you really want to be setting a threshold of 0.5 probability to predict if a patient has cancer or not? The false negative problem: ie the chance you predict someone dosent have cancer who has cancer is much higher for such a threshold. You could kill someone by telling them not to get a biopsy. Why not play it safe and assume a much lower threshold: for eg, if the probability of 1(cancer) is greater than 0.05, we'll call it a 1."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 30, 
      "cell_type": "code", 
      "source": [
        "df=pd.read_csv(\"data/01_heights_weights_genders.csv\")\n", 
        "df.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "from sklearn.linear_model import LogisticRegression\n", 
        "clf_l, Xtrain_l, ytrain_l, Xtest_l, ytest_l  = do_classify(LogisticRegression(), {\"C\": [0.01, 0.1, 1, 10, 100]}, df, ['Height', 'Weight'], 'Gender','Male')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 32, 
      "cell_type": "code", 
      "source": [
        "plt.figure()\n", 
        "ax=plt.gca()\n", 
        "points_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 36, 
      "cell_type": "code", 
      "source": [
        "confusion_matrix(ytest_l, clf_l.predict(Xtest_l))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 37, 
      "cell_type": "code", 
      "source": [
        "def t_repredict(est,t, xtest):\n", 
        "    probs=est.predict_proba(xtest)\n", 
        "    p0 = probs[:,0]\n", 
        "    p1 = probs[:,1]\n", 
        "    ypred = (p1 > t)*1\n", 
        "    return ypred"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 38, 
      "cell_type": "code", 
      "source": [
        "confusion_matrix(ytest_l, t_repredict(clf_l, 0.1, Xtest_l))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## ROC Curve\n", 
        "\n", 
        "The images in this section are from Provost, Foster; Fawcett, Tom (2013-07-27). Data Science for Business: What you need to know about data mining and data-analytic thinking  O'Reilly Media. Great book!\n", 
        "\n", 
        "We can check on the thresholds we talked about in the previous section and compare our classifiers to each other and the baseline models using the ROC curves you learned about in class. \n", 
        "\n", 
        "Remember that ROC curves are actually a set of classifiers, in which we move the threshold for classifying a sample as positive from 1 to 0. Each point on a ROC curve is a separate classifier obtained by considering a different threshold. (In the standard scenario, where we used the  classifier accuracy, this threshold is implicitly set at 0.5, and we have only one classifier).\n", 
        "\n", 
        "![m:roc curve](images/roc-curve.png)\n", 
        "\n", 
        "The way ROC curves are calulated is this. We start with a large threshold, something like 0.99 or so. This means that only samples with a probability of being positive higher than that threshold are classified as positive. That is the really really really positive ones! The idea then is to decrease this threshold, such that more and more samples get classified as positive.\n", 
        "\n", 
        "![howto roc](images/howtoroc.png)\n", 
        "\n", 
        "The practical way to do this is to order the samples by probability of being positive, or in the case of the SVM, by the `decision_function` or distance from the separating hyperplane. Then consider the sample with the highest score or highest probability of being positive. At first, only this sample is positive. Then, we take the sample with the next highest score, and call it positive. As we go down the list, we go down a threshold in score or probability. \n", 
        "\n", 
        "Now, for each such situation: only 1 positive, now 2 positive,....you can imagine a different classifier with a different confusion matrix. It will have its own false positives, tre positives, etc. Its actually the same original classifier, but with a different threshold each time.\n", 
        "\n", 
        "As we keep going down the list, decreasing the threshold, more and more samples become positive, and at first, the true positives rise faster than the false positives. Once past a certain point, false positives increase faster than true positives. Now, if you want a balanced classifier, you look at this turn-around point...the northwest corner, so to speak. But if you want a classifier which penalizes false positives and false negatives differently, the point you want is different.\n", 
        "\n", 
        "Here is the confusion matrix again:\n", 
        "\n", 
        "![hwimages](./images/confusionmatrix.png)\n", 
        "\n", 
        "\n", 
        "To make a ROC curve you plot the True Positive Rate, \n", 
        "\n", 
        "$$TPR=\\frac{TP}{OP}$$\n", 
        "\n", 
        "against the False Positive Rate,\n", 
        "\n", 
        "$$FPR=\\frac{FP}{ON}$$\n", 
        "\n", 
        "as you go through this process of going down the list of samples. ROC curves are useful because they calculate one classifier per threshold and show you where you are in TPR/FPR space without making any assumptions about the utility matrix or which threshold is appropriate.\n", 
        "\n", 
        "Notice that the ROC curve has a very interesting property: if you look at the confusion matrix above, TPR is only calculated from the observed \"1\" row while FPR is calculated from the observed '0' row. This means that the ROC curve is independent of the class balance/imbalance on the test set, and thus works for all ratios of positive to negative samples. The balance picks a point on the curve, as you can read below.\n", 
        "\n", 
        "A rote reading of the ROC curve (go to the \"northwest\" corner) is a bad idea: you must fold in the curve with any assumptions you are making about costs. More on this in the next lab. Still, on the whole, a curve with a greater AUC (area under curve), or further away from the line of randomness, will give us a rough idea of what might be a better classifier.\n", 
        "\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 64, 
      "cell_type": "code", 
      "source": [
        "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n", 
        "    initial=False\n", 
        "    if not ax:\n", 
        "        ax=plt.gca()\n", 
        "        initial=True\n", 
        "    if proba:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n", 
        "    else:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n", 
        "    roc_auc = auc(fpr, tpr)\n", 
        "    if skip:\n", 
        "        l=fpr.shape[0]\n", 
        "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], 'o-', alpha=0.4, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n", 
        "    else:\n", 
        "        ax.plot(fpr, tpr, '.-', alpha=0.2, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n", 
        "    label_kwargs = {}\n", 
        "    label_kwargs['bbox'] = dict(\n", 
        "        boxstyle='round,pad=0.1', alpha=0.1,\n", 
        "    )\n", 
        "    for k in range(0, fpr.shape[0],labe):\n", 
        "        #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n", 
        "        threshold = str(np.round(thresholds[k], 2))\n", 
        "        ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n", 
        "    if initial:\n", 
        "        ax.plot([0, 1], [0, 1], 'k--')\n", 
        "        ax.set_xlim([0.0, 1.0])\n", 
        "        ax.set_ylim([0.0, 1.05])\n", 
        "        ax.set_xlabel('False Positive Rate')\n", 
        "        ax.set_ylabel('True Positive Rate')\n", 
        "        ax.set_title('ROC')\n", 
        "    ax.legend(loc=\"lower right\")\n", 
        "    return ax"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 65, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import roc_curve, auc\n", 
        "ax=make_roc(\"logistic\", clf_l, ytest_l, Xtest_l, labe=100, skip=100)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 66, 
      "cell_type": "code", 
      "source": [
        "sets = dict(Xtrain=Xtrain_l, ytrain=ytrain_l, Xtest=Xtest_l, ytest = ytest_l)\n", 
        "clf_knn, Xtrain_l, ytrain_l, Xtest_l, ytest_l  = do_classify(KNeighborsClassifier(), {\"n_neighbors\": list(range(1,20,1))}, df, ['Height','Weight'], 'Gender', 'Male' , standardize=True, sets=sets)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 74, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import roc_curve, auc\n", 
        "ax=make_roc(\"logistic\", clf_l, ytest_l, Xtest_l, labe=200, skip=80)\n", 
        "make_roc(\"knn\", clf_knn, ytest_l, Xtest_l, ax=ax, labe=200, skip=2)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }
  ]
}